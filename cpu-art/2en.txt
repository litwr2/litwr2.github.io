So differently the same relationships described in different companies - [link]

The segmented method of working with memory was clearly inferior to the paged method in almost all its characteristics.

To manage memory, it became possible to use both large segments up to 4 GB in size and the convenient paged mode.

Due to the fact that the main protected mode became much easier to manage than in the 80286, a number of inherited instructions became unnecessary rudiments.

One can only wonder why Intel so carefully preserves these superfluous cluttering space of opcodes unofficial duplicating instructions?

Interestingly, Apple was going to switch to the SPARC in the second half of the 80's, but negotiations with Sun failed.

Quite often, you can find criticism of memory segmentation, i.e. such an organization that in general, you need to use two pointers to address a memory location.  However, this is a strange criticism, rather contrived.  Segmentation itself is a completely natural way to organize virtualization and memory protection.  In fact, it was not the segmentation itself that was criticized, but only the maximum segment size, 64 KB.  However, this limitation is a direct consequence of the desire to have large amounts of memory when using 16-bit registers.  Therefore, all the criticism of segmentation is actually a disguised requirement to switch to a 32-bit architecture.  The situation was complicated by the fact that segmentation in the first x86 only partially had the functionality of a normal memory management unit, in particular, usage of segment registers was available to application programs.  The 80286 made complete segmentation support available, but this made previous applications for the 8086 incompatible with the mode when this full support was activated.  Only with the introduction of the 80386 were all the problems resolved and the criticism stopped, although the 80386 still used segmentation!

It is surprising that for some reason it is almost impossible to find such criticism regarding the popular PDP-11, where the restrictions on the use of memory are much more stringent and this despite the fact that not the most expensive PDP-11s were significantly more expensive than personal computers and up to the mid-80's faster than the best IBM PC compatible machines.

Using a single pointer to keep the complete address in memory was natural in the architecture of the IBM mainframes, the VAX, and the 68000 processor.  It is easy to notice that this list does not include personal computers, since even the 68000 was originally developed for relatively expensive, non-personal systems.  The 8086 processor retained much in common with the primitive 8080, which was used more as a controller.  Therefore, it is quite strange to compare systems based on the 8088 with, for example, the VAX or even the Sun workstations - these are completely different classes of machines.  But, perhaps, thanks to Bill Gates, the IBM PCs were initially equated with much more expensive systems.  The first IBM PC had only 16 KB of memory, and 64 KB was more of a luxury for an individual customer in 1981.  By the mid-80's, typical memory amounts for the IBM PC compatible systems reached 512 KB - segmentation with such memory amount could almost never create any difficulties.  When the typical memory size for IBM PC compatible machines exceeded 512 KB, the 80386 appeared.  It is worth recalling that even in 1985, most systems were 8-bit and to work with memory amounts of more than 64 KB, you had to use memory bank switching - this is one or even two order of magnitude more difficult and slower than using large arrays with the 8086.  The first IBM PC were quite comparable with 8-bit systems, not with the VAX.  By the way, an alternative design of the IBM PC used the Z80.  Therefore, we can only admire the Intel engineers who have been able to develop the x86 processors for more than 40 years so that they are all the time relatively inexpensive, technically one of the best, and this while maintaining binary compatibility with all previous models, starting with the 8086!  Although this is not a record, IBM has maintained compatibility with the System/360 architecture for almost 60 years.

Support for hardware interrupts in the 6502 is implemented plainly and efficiently.  For masked and non-masked interrupts, two fixed addresses are allocated in memory, where the addresses of the corresponding handlers are written.  About the same, but even simpler, later made the most popular interrupt mode 1 in the Z80.  But software interrupts in the 6502 are implemented quite primitive: they use the address for masked interrupts, which requires an additional software check to distinguish them.  This is why there is a unique software interrupt flag among the 6502 flags.  In addition, the software interrupt instruction has no argument, although such an argument can be added at the cost of complicating the handler procedure.

Among the cheapest VAX-11s, it was known to have compatibility issues. In particular, it was problematic to port Unix to the first VAX-11/730's due to the peculiarities of the implementation of privileged instructions on them.

Only the extended 80186 instruction system and a separate memory management chip were implemented, which should have allowed running programs for the 80286.

Apple, which made the workstation-class Lisa computer, could also be added to this list.

Interestingly, the NMOS 6502 compatible with the 65C02 was never made. Although in the early 80's CMOS technology had no obvious advantages over NMOS and was more expensive.

At the same time, the 80188 appeared, which differed from the 80186 only in a narrow data bus - Intel never forgot about inexpensive solutions for embedded systems.

Consider new instructions for the 80186:
*) single-byte instructions PUSHA and POPA, allowing to save or restore all 8 registers at once;
*) three-operand signed multiplication, unique in the x86 architecture, it is more like an instruction for ARM;
*) bit shifts and rotations, with the argument number - in the 8086, only the number 1 or the CL register can be used.  For argument 1, you can use two types of instructions: fast and short, inherited from the 8086, or generalized, longer and slower for any numeric arguments, which is rather useless;
*) string commands for working with i/o ports, they are somewhat more powerful than similar ones available in the Z80;
*) the ENTER and LEAVE instructions - support for working with subroutines in high-level languages.  They know how to work with syntactic nesting of subroutines up to 32 levels - the use of this type of nesting is typical for Pascal language.  However, for Pascal, you probably cannot find a single program where the nesting would be more than 3.  And Pascal itself has been used less and less since then. Here you can see that Motorola also added Pascal support to the 68020, which was later recalled with regret;
*) the BOUND command to check whether the array index is valid.

Interestingly, the 80386 development team was distinguished by a peculiar and overt religiosity.

The built-in cache appeared to be an innovation even though it was a small 256 bytes of capacity, which allowed sometimes to significantly improve the performance because the main dynamic memory could not keep up with the processor.  Although in the general case, such a small cache only slightly affected the performance. 

As an example of an unusual command, you can also point to multiplication, in which, depending on the number of the register used, for the result, you can get either the full 32-bit product, or only its lower 16 bits.

Made in USSR 16-bit home computer (model of 1987) â€“ it is almost PDP-11 compatible

But when compared with other architectures, PDP-11 sometimes shows even better code density in practice!


