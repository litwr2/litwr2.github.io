<head>
<meta charset=utf-8>
<title>Оптимизация быстрой сортировки</title>
<meta name=keywords content="C++, sorting algorithm, insertion sort, shell sort, quick sort, introspective sort, intro sort, heap sort,benchmarks, programming, sorting algorithms, stl, bsd, strings, data structures, std::sort, std::qsort, lomuto sort, dualpivot sort, gcc">
<style type=text/css>
pre, textarea {
    background-color: lightgreen;
}
table, th, td {
   border-width:thin;
   border-style:solid;
   border-color:green;
   border-spacing:0px;
}
</style>
</head>
<body>
<h1>Quicksort optimization</h1>

<p>Quicksort, discovered at the turn of the 60s by Tony Hoare, is known for its ease of implementation and versatility.  However, the main thing that makes it attractive is its high efficiency.  In the absolute majority of cases the speed of its execution is close to record indicators: the time of its execution in such cases is determined by the formula N*Log(N), where N is the number of elements to be sorted.  Therefore, for a long time quicksort was considered the most standard sorting method.  The latter is reflected in the fact that the standard C library has qsort function (std::qsort in C++), which as a rule is an implementation of quicksort.  The more modern standard sort, std::sort, implements introspective sorting, which is based on quicksort.  Java also uses modern quicksort as the default sort for non-object data.

<p>Another attractive feature of quicksort is that it doesn't need to allocate additional memory when sorting data.  This compares favorably with many sorting methods, such as mergesort, radixsort or the now popular timsort.  But quicksort is recursive and therefore uses a stack intensively, in which there must be enough room for it to work.  In the vast majority of cases quicksort requires a relatively small stack size proportional to the binary logarithm of the number of elements being sorted.

<p>The whole quicksort algorithm can be described in a few sentences.  The sequence to be sorted is divided into two parts in such a way that in the first part all elements to be sorted are less (or more) than in the second part.  The process is repeated with each part.  That's the whole algorithm!  This partitioning is usually made based on the selected pivot &ndash; larger elements go to one part, and smaller ones to the other.  Hoar suggested selecting a pivot from the middle of the sorted sequence, but it can be selected in other ways as well.

<p>However, quick sort has two serious drawbacks.  The most important of them is that on some extremely rare almost statistically impossible data sequences, quicksort works very slowly, at the level of the simplest and slowest sorting algorithms, according to the quadratic law.  In such rare cases, for example, instead of the expected fractions of a second sorting can take hours, which can break a system where this happens.  The second drawback is that on such very rare data sequences, quicksort can use a stack size proportional to the number of elements being sorted, and in this case a standard stack allocated to the task may not be enough, which will cause the execution of such a task to crash.  For example, on modern systems Lomuto's sort is almost guaranteed to break if it sorts a million ordered or nearly ordered data items.  Therefore, in particular quicksort cannot be used in OS kernel mode, where the stack size is small.

<p>By the way, when using permutations of N different elements, there is a formula that allows us to calculate the number of sequences that make quicksort run at the lowest speed and also use the maximum stack volume.  It's 2<sup>N-3</sup>.  The total number of all such permutations is obviously equal to N! &ndash; it is the factorial of N.  There are still a much larger number of cases where quicksort does not get much faster, but the total number of such cases of such severe slowing is still very small when compared to the total number of cases. If we consider all possible cases of sequences of N elements, where each element can take M values, then the total number of cases will be M<sup>N</sup>. Unfortunately, I could not find an exact formula for the number of bad sequences in this most general case. Of course, there are significantly more of them than in the case of using permutations from different elements, but it is practically well known that this is also very small, almost zero percent of the total number of cases.

<p>Given the attractive features of quicksort, there are constant attempts to improve it.  Nearly all improvements fall into two main categories.  The first includes attempts to more optimally select the pivot, the second is to minimize stack usage.

<p>Several typical ways to select a pivot have been suggested.  Ideally, the pivot should split the data into two approximately equal parts, but to accurately find such an element you need to process almost all data for sorting (ideally, they need to be sorted!), which will slow down the work unacceptably.  In addition, the selection of the pivot should be made as quickly as possible.  Therefore, we get two mutually contradictory requirements and must look for the best option.  For simplicity and maximum speed you can choose one of the edge elements.  This is done in the popular <a>version</a> of quicksort proposed by Lomuto, which is therefore on average slightly faster than the classical version of Hoare on random data.  But a simple selection method makes sorting more vulnerable, for example, Lomuto sorting is guaranteed to be quadratic by simple ordered or almost ordered data.  Hoare sorting is not so easy to break, for it you need to design special non-trivial breaking sequences.  For the tests, I used two such sequences that slow down Hoare's sort by choosing one-element partitions on the right and left.  A more advanced method, compared to Hoare's method consists in choosing the best of the three selected elements, their median.  Usually the median of the first, middle and last elements is taken.  Monte Carlo testing has shown that the average pivot when using the median of three is about a third closer to the ideal median than when using just the middle element.  Quite fast methods have been found for calculating the median of three (surprisingly, fast choosing the middle from three elements is not a completely trivial task).  This is why the median of three is used in the std::sort and std::qsort (gcc) methods.  My tests confirm that this choice of pivot is on average better than Hoare's method, but only slightly &ndash; the cost of calculating the median is almost equal to the benefits from using it.  The choice of a pivot element with a median of five (seven or more) data elements is too slow and also does not guarantee safety if a bad sequence has to be sorted.  To guarantee the escape from the possibility of quadratic processing, the choice of the median of the medians for the pivot is used: the sorted sequence is usually divided into five parts, in which the median is recursively found, and then the median of the selected five medians is taken.  This method ensures that in the worst case, the pivot will split the original sequence into parts approximately in a ratio of 30% to 70%, which guarantees that the quadratic case is impossible.  But the calculation of the median of the medians itself makes quicksort several times slower, therefore, it is better to calculate the pivot element with such a relatively slow method only if the number of recursive calls has become larger than the specified limit.  Another approach to choosing a pivot is its randomization, which allows you to effectively deal with cases of specially designed sequences that break quicksort.

<p>There are two ways to minimize stack usage.  It is possible to guarantee a logarithmic amount of stack memory required by placing the smaller portion of the sequence ahead of the larger one and then optimizing the tail call, leaving only one of the two recursive calls, which for some sequences does not require recursive calls at all.  Tail call optimization makes the code slightly faster.  You can completely eliminate recursion by storing data in a specially created stack, which will reduce the load on such a stack by a third.  This is how std::qsort (gcc) is made in the current implementation.  It is also worth emphasizing that simply optimizing the tail call will only reduce the likelihood of a case where the sort may not have enough stack space.

<p>Let's now figure out which optimizations are worth using and which are most likely not.  You should not be too smart with the choice of a pivot element, since a more complex choice requires additional calculations that will absorb the benefits of such a better choice.  Performance tests show that simple pivot selection methods provide faster performance on typical random data.  The only way to reduce the chance of very slow, quadratic processing is by analyzing a larger amount of data when choosing a pivot.  Randomizing this choice will only hide the problem; it will not diminish the probability.  In addition, accessing the random number generator is a relatively slow operation.  Thus, it is impossible to get rid of the possibility of a quadratic time dependence when using the quicksort algorithm, without processing a significant part of the data, which will turn the sorting into a relatively slow one.

<p>When optimizing stack usage, keep in mind that this does not protect against cases of quadratic running time.  For example, I managed to kill std::qsort.  Eliminating the second recursive call makes it impossible to detect all situations when sorting shows a tendency to quadratic processing by analyzing the value of the stack pointer.  Although direct work with the stack pointer is normal only for system programs; for ordinary, applied ones, it is easier to use a special variable to store the depth of a recursive call &ndash; this is how, through a variable, the depth of recursive calls is tracked in std::sort.  To eliminate the possibility of quadratic processing, std::sort checks the value of this variable and if it exceeds the allowed value, the algorithm switches to heap sorting. By the way on modern hardware, shellsort on scalar data is noticeably faster than heapsort, so it is possible that it is better to switch to shellsort for numbers.

<p>The idea of switching to another way of sorting is impressive in its reliability and simplicity.  However, it also seems a bit cumbersome.  As an alternative, you can suggest simply restarting quicksort.  This works because the data is partially ordered during the recursive calls.  Of course, the Lomuto's variant will not work here, but the original Hoare's variant with a middle pivot element or a typical median of three works great in this case.

<p>A special category of quicksort optimization is its replacement with a more efficient one for a small number of sorted elements.  Usually, insertionsort is used for this purpose.  This significantly improves performance.

<p>The most interesting optimizations involve something completely new.  So Vladimir Yaroslavsky proposed in 2009 to use divisions not into two, but into three parts, which required the use of two pivots. This really has given an increase in performance.  His sorting, as I have already written about, is already used in Java.  If you integrate it into introsort, then the latter has also to speed up.  For some reason, GCC people are still holding back on this.  Interestingly, you can split data into two parts without using a pivot at all.  This <a>type</a> of quicksort has been known for a long time, its characteristics are close to that of Lomuto.

<p>Modern compilers can surprise with their sometimes hard to predict optimizations.  It is not uncommon for code that has been manually optimized with difficulty to run slower than the original code.  For example, I've found that manual tail recursion optimization is often slower than automatic!  Suddenly, it turned out that calls (only calls!) to std::sort after compilation with and without optimization are completely different things, and the first call is about 10 times faster!  It can even be assumed that the std::sort code is somehow specially tailored to the optimizer &ndash; there for example, even a call to heapsorting doesn't go directly, but through a special function available to developers, but absent in the documentation for the standard library.

<p>In conclusion, here is the <a>code</a> for a safe quicksort.  It is based on the classic Hoare quicksort, but on small arrays it switches to insertionsort, and if the depth of recursive calls becomes greater than the specified one, it simply restarts.  When compiled without optimization, it outperforms std::sort, and it almost always outperforms std::qsort on numbers.  It can even outperform a double pivot sort if the records are sorted by the value of one of their fields.  The safe quicksort can also be used in the OS kernel by explicitly specifying the maximum number for recursive calls.  It is still noticeably faster than shellsort, which is used by default in some OS kernels.  Obviously, in a similar way it is possible to turn into safe and V. Yaroslavsky's sorting.  In Java, catch and throw can be used instead of straightforward setjmp and longjmp.

<p>Of course, the question arises, how reliable is this method?  Is there any loophole here for the possibility of working according to the quadratic law?  Through massive testing, I almost never managed to make this sort work slower than heapsort: heapsort turned out to be faster only in cases of working with text strings in an order that breaks Hoare's sort.  Keep in mind however, that the safe quicksort can actually be looped if you specify a number of recursive calls less than the binary logarithm of the data size.  So far I have set the default value to a number about 10 times larger.

<p>Data on the speed of safe quick sort can be found in the following interactive table.

<p>Perhaps I missed something. I would be glad to receive comments and additional information.

<br>
<br>
<hr>
A version of the article is published on <a>habrahabr</a>
<br>
<br>
